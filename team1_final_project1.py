# -*- coding: utf-8 -*-
"""team1_final_project1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IUmsWZ8YCh_uh8A41TbopqwpjknLB7ZG

### Final Project Skeleton Code
This notebook is meant to be skeleton code for the final project. We have filled in the code to allow you to download the dataset and train your neural network on it. We have not made the neural network for you. All of the specifics, including layer sizes and hyperparameters, are up to you to decide. Some of this code may not work perfectly for you, so you will have to read it and understand what is going on so that you can create your neural network. The testing cell at the end is how we will be testing your neural networks, so be sure that whatever you do, it works with that cell. (For example: do any data processing such as flattening and padding in the forward function rather than in the testing loop as we will not be editing our testing loop to reflect any of those changes)
"""

!pip install torch

"""### Import Libraries"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import numpy as np
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/gdrive')

"""### Download training and testing data"""

transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5), (0.5))])

trainset = torchvision.datasets.MNIST('PATH_FOR_TRAINING_SET', download=True, train=True, transform=transform)
testset = torchvision.datasets.MNIST('PATH_FOR_TESTING_SET', download=True, train=False, transform=transform)

trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)
testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True)

"""### Create dataloaders and visualize some examples"""

dataiter = iter(trainloader)
images, labels = dataiter.next()

print(images.shape)
print(labels.shape)

figure = plt.figure()
num_of_images = 60
for index in range(1, num_of_images + 1):
    plt.subplot(6, 10, index)
    plt.axis('off')
    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')

"""### Model Initialization"""

#This is your neural network class, so it must extend nn.Module
#For your final submission you will be submitting this cell as its own file
class Digit_Classifier(nn.Module):
  def __init__(self):
    #Handle some under-the-hood PyTorch stuff
    super().__init__()

    #Now put your layers below in addition to any other member variables you need
    #
    self.conv1 = nn.Conv2d(1, 10, 4)
    self.pool = nn.MaxPool2d(2, 2)
    self.conv2 = nn.Conv2d(10,16,2)
    self.pool2 = nn.MaxPool2d(2,2)
    self.conv3 = nn.Conv2d(16, 20, 2)
    self.fc1 = nn.Linear(20*2*2, 40)
    self.fc2 = nn.Linear(40, 16)
    self.fc3 = nn.Linear(16, 10)
  def forward(self, x):
    #Now here you add your forward pass, e.g. how the layers fit together
    #Tips:
    # 1. Don't forget the ReLU layer when needed
    # 2. Consider normalization
    # 3. If you are getting errors look at your dimensions, dimension errors are very easy to make!
    # 4. CNN layers take in rectangular (or square) inputs and give rectangular (or square) outputs. Fully connected layers have input and output that are vectors, when you need to switch between the two consider using a flatten or reshape
    x = self.pool(F.relu(self.conv1(x)))
    x = self.pool(F.relu(self.conv2(x)))
    x = self.pool(F.relu(self.conv3(x)))
    x = x.view(-1, 20*2*2)
    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    x = self.fc3(x)
    return x

  #Optional: any other member functions that you think would be helpful

#Instantiate your model:
model = Digit_Classifier()

"""### Hyperparameters Tuning"""

#hyperparameters: currently all are set to 1, it is up to you to figure out what they should be
#Don't just randomly change the hyperparameters: consider what each of them are changing (look in the code to find where the actually affect things)
num_epochs = 10
learning_rate = 0.001
momentum = 0
criterion = nn.CrossEntropyLoss() #consider using other loss functions: https://pytorch.org/docs/stable/nn.html#loss-functions
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum) #consider using other optimizers: https://pytorch.org/docs/stable/optim.html#algorithms

"""### Model Training"""

#Consider how you will keep track of losses, it would be great to have graphs of your loss over time in your final presentation
running_loss = []

#Training loop
model.train()
for i in range(num_epochs):
  for images, labels in trainloader:
    # do any preprocessing, for example flattening if needed
    #print(images.shape)
    #zero out the gradients
    optimizer.zero_grad()
    #Calculate the output
    output = model(images)

    #Calculate the loss of the output, note: labels are the ACTUAL image labels, whereas output are your models guesses
    loss = criterion(output, labels)
    running_loss.append(loss.detach().numpy())

    #Backpropagation (this is the part where we take the gradients [multivariable derivatives] of all the weights)
    loss.backward()

    #This is the part where we actually update the weights: how is learning_rate related to this step?
    optimizer.step()

    #Consider adding some checking here to see how your loss has been doing over time, this will save you a lot of time if you can notice immediate issues
    #While this is not required it is HIGHLY recommended, try to do it yourself but reach out if you need help

plt.plot(running_loss)

"""### Save your model"""

#Save your model weights (this will generate a file that you will have to submit to us)
torch.save(model.state_dict(), '/content/gdrive/MyDrive/Colab Notebooks/Untitled folder/weights2.pth')

"""### Evaluate Test Accuracy"""

model.eval()
correct = 0
total = 0

#ensure gradients won't get changed
with torch.no_grad():
  for images, labels in testloader:
    for i in range(len(labels)):
      #do any preprocessing

      #calculate the output
      output = model(images[i])
      print(labels)
      print(output)

      predicted_label = torch.argmax(output, dim=1)

      if predicted_label == labels[i]:
        correct += 1
      total += 1
      break

print("Test accuracy: ", correct/total)